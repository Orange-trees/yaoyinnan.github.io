<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>scrapy爬取知名技术文章网站 | Yaoyinnan's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">scrapy爬取知名技术文章网站</h1><a id="logo" href="/.">Yaoyinnan's Blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">scrapy爬取知名技术文章网站</h1><div class="post-meta">May 31, 2018</div><div class="post-content"><h2 id="确认爬取目标"><a href="#确认爬取目标" class="headerlink" title="确认爬取目标"></a>确认爬取目标</h2><p>明确网站（伯乐在线）的结构</p>
<p>比如说最新文章的url：<a href="http://blog.jobbole.com/all-posts/page/2/" target="_blank" rel="noopener">http://blog.jobbole.com/all-posts/page/2/</a></p>
<p>每一页的位置和最后的页码有关</p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/1.png" alt=""></p>
<p>方案1：通过页码的调整，存储文章列表所有url。（缺点：无法时时获得总页数）</p>
<p>方案2：通过判断是否有“下一页”的按钮，通过不断点击下一页来获取url。</p>
<h2 id="新建爬虫项目"><a href="#新建爬虫项目" class="headerlink" title="新建爬虫项目"></a>新建爬虫项目</h2><p>1.新建一个python虚拟环境（article_spider）</p>
<p>进入虚拟环境，进行下列2、3、4操作：</p>
<p>2.安装scrapy（可用豆瓣源）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install -i https://pypi.douban.com/simple/ scrapy</span></span><br></pre></td></tr></table></figure>
<p>3.在项目文件夹中创建爬虫项目（ArticleSpider）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scrapy startproject ArticleSpider</span></span><br></pre></td></tr></table></figure>
<p>4.在项目中创建爬虫模板（可自定义）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ArticleSpider</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> scrapy genspider example example.com</span></span><br></pre></td></tr></table></figure>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/2.png" alt=""></p>
<p>5.导入article_spider虚拟环境</p>
<p>setting-&gt;interpreter–&gt;Add Local–&gt;虚拟环境</p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/3.png" alt=""></p>
<blockquote>
<p>注意：当点击右上角设置锯齿图标时，show all 后若有多个重名解释器时会提示错误</p>
</blockquote>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/error.png" alt=""></p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>在根目录下创建main.py文件用于调试</p>
<p>main.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"yaoyinnan"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</span><br><span class="line">execute([<span class="string">"scrapy"</span>, <span class="string">"crawl"</span>, <span class="string">"jobbole"</span>])</span><br></pre></td></tr></table></figure>
<p>需要注意的是，在settings.py文件中将<strong>ROBOTSTXT_OBEY</strong>设置为False，否则的话爬虫会默认识别网站是否遵循ROBO协议，只爬去遵循的，爬虫会很快停止。</p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/4.png" alt=""></p>
<p>打断点，debug看response返回的内容</p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/5.png" alt=""></p>
<h2 id="xpath"><a href="#xpath" class="headerlink" title="xpath"></a>xpath</h2><h4 id="xpath简介"><a href="#xpath简介" class="headerlink" title="xpath简介"></a>xpath简介</h4><p>1.xpath使用路径表达式在xml和html中进行导航</p>
<p>2.xpath包含标准函数库</p>
<p>3.xpath是一个w3c的标准</p>
<h4 id="xpath节点关系"><a href="#xpath节点关系" class="headerlink" title="xpath节点关系"></a>xpath节点关系</h4><p>1.父节点</p>
<p>2.子节点</p>
<p>3.同胞结点</p>
<p>4.先辈结点</p>
<p>5.后代结点</p>
<h4 id="xpath语法"><a href="#xpath语法" class="headerlink" title="xpath语法"></a>xpath语法</h4><p>所有下标从1开始</p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/6.png" alt=""></p>
<blockquote>
<p>/ 一层</p>
<p>// 无论多少层</p>
<p>@ 属性</p>
</blockquote>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/7.png" alt=""></p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/8.png" alt=""></p>
<blockquote>
<p>| 求并集</p>
</blockquote>
<h2 id="应用xpath编写逻辑代码"><a href="#应用xpath编写逻辑代码" class="headerlink" title="应用xpath编写逻辑代码"></a>应用xpath编写逻辑代码</h2><p>jobbole.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'jobbole'</span></span><br><span class="line">    allowed_domains = [<span class="string">'blog.jobbole.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/113942/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        re_selector1 = response.xpath(<span class="string">"/html/body/div[1]/div[3]/div[1]/div[1]/h1/text()"</span>)</span><br><span class="line">        re_selector2 = response.xpath(<span class="string">'//*[@id="post-113942"]/div[1]/h1/text()'</span>)</span><br><span class="line">        re_selector3 = response.xpath(<span class="string">'//*[@class="entry-header"]/h1/text()'</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>通过检查–&gt;右键–&gt;copy path–&gt;获得网页结构xpath</p>
<p>方法一：“绝对path”，从头开始一级一级往下写xpath</p>
<p>方法二：通过ID，每个id都是固定的，用离着所需标签最短的id缩短xpath</p>
<p>方法三：通过不重复的Class，方式同ID</p>
<blockquote>
<p>注意：这里的“第几个div”是从1开始数，和数组从0开始不一致。</p>
</blockquote>
<p>通过python每次debug太慢了，可以通过shell命令进行爬取</p>
<p>先进入虚拟环境下的项目中，再输入：</p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/10.png" alt=""></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scrapy shell url</span></span><br></pre></td></tr></table></figure>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/11.png" alt=""></p>
<p>在shell中成功的代码要复制回pycharm</p>
<p>进一步爬取：title、time、favour</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'jobbole'</span></span><br><span class="line">    allowed_domains = [<span class="string">'blog.jobbole.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/113942/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        title = response.xpath(<span class="string">'//*[@class="entry-header"]/h1/text()'</span>).extract()[<span class="number">0</span>]</span><br><span class="line">        create_date = response.xpath(<span class="string">'//*[@id="post-113942"]/div[2]/p/text()'</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>, <span class="string">""</span>).strip()</span><br><span class="line">        favour = response.xpath(<span class="string">'//*[@id="113942votetotal"]/text()'</span>).extract()[<span class="number">0</span>] + <span class="string">"赞"</span></span><br><span class="line"></span><br><span class="line">        print(title)</span><br><span class="line">        print(create_date)</span><br><span class="line">        print(favour)</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>extract() : 提取，返回的是一个数组</p>
<p>extract()[0] :  数组的第一个元素</p>
<blockquote>
<p>可以用extract_first()，取第零个上面数组下标的方式获取时需要做异常处理，而这种无需。</p>
<blockquote>
<p>并且可以加参数，参数作为提取不到的返回值</p>
</blockquote>
</blockquote>
<p>replace() : 替换</p>
<p>strip() ： 移出字符串尾指定字符，默认为空格</p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'jobbole'</span></span><br><span class="line">    allowed_domains = [<span class="string">'blog.jobbole.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/113942/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 标题</span></span><br><span class="line">        title = response.xpath(<span class="string">'//*[@class="entry-header"]/h1/text()'</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 时间</span></span><br><span class="line">        create_date = response.xpath(<span class="string">'//*[@id="post-113954"]/div[2]/p/text()'</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>,</span><br><span class="line">                                                                                                            <span class="string">""</span>).strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 内容</span></span><br><span class="line">        content = response.xpath(<span class="string">'//div[@class="entry"]'</span>).extract()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 关键词</span></span><br><span class="line">        tag_list = response.xpath(<span class="string">'//p[@class="entry-meta-hide-on-mobile"]/a/text()'</span>).extract()</span><br><span class="line">        tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</span><br><span class="line">        tags = <span class="string">","</span>.join(tag_list)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 赞</span></span><br><span class="line">        praise_nums = response.xpath(<span class="string">'//*[@id="113954votetotal"]/text()'</span>).extract_first() + <span class="string">"赞"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 收藏</span></span><br><span class="line">        collection_str = response.xpath(<span class="string">'//span[contains(@class,"bookmark-btn")]/text()'</span>).extract_first()</span><br><span class="line">        regex_col = <span class="string">"(.[0-9])"</span></span><br><span class="line">        collection_str = re.match(regex_col, collection_str).group(<span class="number">0</span>) + <span class="string">"收藏"</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 评论</span></span><br><span class="line">        comment_str = response.xpath(<span class="string">'//div[contains(@class, "post-adds")]/a/span/text()'</span>).extract_first()</span><br><span class="line">        regex_col = <span class="string">"(.[0-9])"</span></span><br><span class="line">        comment_str = re.match(regex_col, comment_str).group(<span class="number">0</span>) + <span class="string">"评论"</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/12.png" alt=""></p>
<p>去除评论：[element for element in tag_list if not element.strip().endswith(“评论”)]</p>
<p>用“,”连接数组元素：”,”.join(tag_list)</p>
<h2 id="CSS选择器"><a href="#CSS选择器" class="headerlink" title="CSS选择器"></a>CSS选择器</h2><p><img src="/2018/05/31/scrapy爬取知名技术文章网站/13.png" alt=""></p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/14.png" alt=""></p>
<p><img src="/2018/05/31/scrapy爬取知名技术文章网站/15.png" alt=""></p>
<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobboleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'jobbole'</span></span><br><span class="line">    allowed_domains = [<span class="string">'blog.jobbole.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://blog.jobbole.com/113942/'</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="comment"># 标题</span></span><br><span class="line">    title = response.xpath(<span class="string">'//*[@class="entry-header"]/h1/text()'</span>).extract()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 时间</span></span><br><span class="line">    create_date = response.xpath(<span class="string">'//*[@id="post-113954"]/div[2]/p/text()'</span>).extract()[<span class="number">0</span>].strip().replace(<span class="string">"·"</span>,</span><br><span class="line">                                                                                                        <span class="string">""</span>).strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 内容</span></span><br><span class="line">    content = response.xpath(<span class="string">'//div[@class="entry"]'</span>).extract()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关键词</span></span><br><span class="line">    tag_list = response.xpath(<span class="string">'//p[@class="entry-meta-hide-on-mobile"]/a/text()'</span>).extract()</span><br><span class="line">    tag_list = [element <span class="keyword">for</span> element <span class="keyword">in</span> tag_list <span class="keyword">if</span> <span class="keyword">not</span> element.strip().endswith(<span class="string">"评论"</span>)]</span><br><span class="line">    tags = <span class="string">","</span>.join(tag_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 赞</span></span><br><span class="line">    praise_nums = response.xpath(<span class="string">'//*[@id="113954votetotal"]/text()'</span>).extract_first() + <span class="string">"赞"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 收藏</span></span><br><span class="line">    collection_str = response.css(<span class="string">'.bookmark-btn::text'</span>).extract_first()</span><br><span class="line">    regex_col = <span class="string">"(.[0-9])"</span></span><br><span class="line">    collection_str = re.match(regex_col, collection_str).group(<span class="number">0</span>) + <span class="string">"收藏"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 评论</span></span><br><span class="line">    comment_str = response.css(<span class="string">'a .btn-bluet-bigger::text'</span>).extract_first()</span><br><span class="line">    regex_col = <span class="string">"(.[0-9])"</span></span><br><span class="line">    comment_str = re.match(regex_col, comment_str).group(<span class="number">0</span>) + <span class="string">"评论"</span></span><br></pre></td></tr></table></figure>
<p>通过css选择器可以更加方便的爬取。</p>
<h2 id="爬取完整文章列表"><a href="#爬取完整文章列表" class="headerlink" title="爬取完整文章列表"></a>爬取完整文章列表</h2><h4 id="解析列表页所有文章url"><a href="#解析列表页所有文章url" class="headerlink" title="解析列表页所有文章url"></a>解析列表页所有文章url</h4><p>首先，改变start_urls为列表页url。</p>
<h5 id="第一步："><a href="#第一步：" class="headerlink" title="第一步："></a>第一步：</h5><p>通过分析得到列表页所有文章url的list：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">post_urls = response.css(<span class="string">"#archive .floated-thumb .post-thumb a::attr(href)"</span>).extract()</span><br></pre></td></tr></table></figure>
<p>其中attr(href)可以获取对应html标签的属性的值。</p>
<h5 id="第二步："><a href="#第二步：" class="headerlink" title="第二步："></a>第二步：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> post_url <span class="keyword">in</span> post_urls:</span><br><span class="line">    <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, post_url), callback=self.parse_detail)</span><br></pre></td></tr></table></figure>
<p>通过遍历每一条url，并且将其交给scrapy下载。</p>
<p><strong>yield</strong>：下载</p>
<p><strong>Request方法</strong>：</p>
<p>需要调用scrapy.http</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request</span><br></pre></td></tr></table></figure>
<p>它可以进入url页面，再通过之前编写的解析代码获取其中内容。</p>
<p>其中可以指定url和callback的函数：</p>
<p>callback函数需要指定为该url回调的函数，比如目前解析列表页所有文章url，则需要回调到之前编写的parse_detail解析文章页面函数。</p>
<p><strong>parse.urljoin方法</strong>：因为有的网站的a标签里的href不存放完整的url，需要通过一定的方法将其与域名链接。</p>
<p>需要调用urlib</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br></pre></td></tr></table></figure>
<h4 id="提取下一页"><a href="#提取下一页" class="headerlink" title="提取下一页"></a>提取下一页</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">next_urls = response.css(<span class="string">".next.page-numbers::attr(href)"</span>).extract_first(<span class="string">""</span>)</span><br><span class="line"><span class="keyword">if</span> next_urls:</span><br><span class="line">    <span class="keyword">yield</span> Request(url=parse.urljoin(response.url, next_urls), callback=self.parse)</span><br></pre></td></tr></table></figure>
<p>与之前思路相仿，将下一页的url提取出来，回调给本函数即可。</p>
<p>由此过程，便提取了文章列表所有文章的内容，下一步需要将其存入数据库中。</p>
<h2 id="存入数据库"><a href="#存入数据库" class="headerlink" title="存入数据库"></a>存入数据库</h2><h4 id="借助Items格式化存储字段"><a href="#借助Items格式化存储字段" class="headerlink" title="借助Items格式化存储字段"></a>借助Items格式化存储字段</h4><p>首先，再<strong>items.py</strong>中定义自己的Item类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    url_object_id = scrapy.Field()</span><br><span class="line">    front_image_url = scrapy.Field()</span><br><span class="line">    front_image_path = scrapy.Field()</span><br><span class="line">    praise_nums = scrapy.Field()</span><br><span class="line">    collection_nums = scrapy.Field()</span><br><span class="line">    comment_nums = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>回到<strong>jobboleList.py</strong>，由于存储在收藏数和评论数是int类型，需要做判断：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 收藏</span></span><br><span class="line">collection_str = response.css(<span class="string">'.bookmark-btn::text'</span>).extract_first()</span><br><span class="line">regex_col = <span class="string">"(.[0-9])"</span></span><br><span class="line">match_re = re.match(regex_col, collection_str)</span><br><span class="line"><span class="keyword">if</span> match_re:</span><br><span class="line">    collection_nums = int(match_re.group(<span class="number">0</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    collection_nums = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 评论</span></span><br><span class="line">comment_str = response.css(<span class="string">'a .btn-bluet-bigger::text'</span>).extract_first()</span><br><span class="line">regex_col = <span class="string">"(.[0-9])"</span></span><br><span class="line">match_re = re.match(regex_col, comment_str)</span><br><span class="line"><span class="keyword">if</span> match_re:</span><br><span class="line">    comment_nums = int(match_re.group(<span class="number">0</span>))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    comment_nums = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>再<strong>jobboleList.py</strong>中引入之前定义好的Items类，并对其实例化：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ArticleSpider.items <span class="keyword">import</span> JobBoleArticleItem</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> ...</span></span><br><span class="line"><span class="function">	...</span></span><br><span class="line"><span class="function">	...</span></span><br><span class="line">	article_item = JobBoleArticleItem()</span><br></pre></td></tr></table></figure>
<p>通过实例的items对象，将每篇文章页爬取得字段保存在items对象中进行后续存储操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将id格式化为md5格式</span></span><br><span class="line">article_item[<span class="string">"url_object_id"</span>] = get_md5(response.url)</span><br><span class="line"></span><br><span class="line">article_item[<span class="string">"title"</span>] = title</span><br><span class="line">article_item[<span class="string">"url"</span>] = response.url</span><br><span class="line">article_item[<span class="string">"create_date"</span>] = create_date</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将提取的日期转化为日期对象</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    create_date = datetime.datetime.strptime(create_date, <span class="string">"%Y/%m/%d"</span>).date()</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    create_date = datetime.datetime.now().date()</span><br><span class="line"></span><br><span class="line">article_item[<span class="string">"front_image_url"</span>] = [front_image_url]	<span class="comment"># 需要注意，这里传入的是数组！</span></span><br><span class="line"></span><br><span class="line">article_item[<span class="string">"praise_nums"</span>] = praise_nums</span><br><span class="line">article_item[<span class="string">"collection_nums"</span>] = collection_nums</span><br><span class="line">article_item[<span class="string">"comment_nums"</span>] = comment_nums</span><br><span class="line">article_item[<span class="string">"tags"</span>] = tags</span><br><span class="line">article_item[<span class="string">"content"</span>] = content</span><br><span class="line"></span><br><span class="line"><span class="keyword">yield</span> article_item</span><br></pre></td></tr></table></figure>
<p>md5需要引入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ArticleSpider.utils.common <span class="keyword">import</span> get_md5</span><br></pre></td></tr></table></figure>
<p>date对象需要引入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br></pre></td></tr></table></figure>
<h4 id="提取图片"><a href="#提取图片" class="headerlink" title="提取图片"></a>提取图片</h4><p>在<strong>pipelines.py</strong>中定义<strong>ArticleImagePipeline</strong>方法作为管道（需要引入ImagesPipeline）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br></pre></td></tr></table></figure>
<p><strong>ArticleImagePipeline</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</span><br><span class="line">            image_file_path = value[<span class="string">"path"</span>]</span><br><span class="line">        item[<span class="string">"front_image_path"</span>] = image_file_path</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>注意需要<strong>return item</strong>，以备下一个item传入。</p>
<p>在<strong>setting.py</strong>中开启<strong>ITEM_PIPELINES</strong></p>
<p>先引入os：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure>
<p><strong>ITEM_PIPELINES</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="comment"># 获取image的path</span></span><br><span class="line">    <span class="string">'ArticleSpider.pipelines.ArticleImagePipeline'</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="comment"># 下载图片</span></span><br><span class="line">    <span class="string">'scrapy.pipelines.images.ImagesPipeline'</span>: <span class="number">3</span></span><br><span class="line">&#125;</span><br><span class="line">IMAGES_URLS_FIELD = <span class="string">"front_image_url"</span>   <span class="comment"># 确认所下载字段</span></span><br><span class="line">project_dir = os.path.abspath(os.path.dirname(__file__))   <span class="comment"># 获取当前目录路径</span></span><br><span class="line">IMAGES_STORE = os.path.join(project_dir, <span class="string">"images"</span>)   <span class="comment"># join连接</span></span><br></pre></td></tr></table></figure>
<p>items流经的pipelines（管道），数字代表管道流经顺序（数字越小越先执行）。</p>
<h4 id="存入Json文件"><a href="#存入Json文件" class="headerlink" title="存入Json文件"></a>存入Json文件</h4><p>在pipelines.py中引入json</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></table></figure>
<p>通过定义“管道“将items对象保存的字段存储如json文件。</p>
<h5 id="自定义Json文件的导出"><a href="#自定义Json文件的导出" class="headerlink" title="自定义Json文件的导出"></a>自定义Json文件的导出</h5><p>首先在<strong>pipelines.py</strong>中定义<strong>JsonWithEncodingPipeline</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWithEncodingPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 自定义json文件的导出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = codecs.open(<span class="string">'article.json'</span>, <span class="string">'w'</span>, encoding=<span class="string">"utf-8"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        lines = json.dumps(dict(item), ensure_ascii=<span class="keyword">False</span>) + <span class="string">'\n'</span></span><br><span class="line">        self.file.write(lines)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure>
<p>存入文件article.json</p>
<p>自定义方法存入的内容可能会有一定的问题和错误，需要注意细节。</p>
<h5 id="调用scrapy提供的json-export导出json文件"><a href="#调用scrapy提供的json-export导出json文件" class="headerlink" title="调用scrapy提供的json_export导出json文件"></a>调用scrapy提供的json_export导出json文件</h5><p>首先引入<strong>JsonItemExporter</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exporters <span class="keyword">import</span> JsonItemExporter</span><br></pre></td></tr></table></figure>
<p>在<strong>pipelines.py</strong>中定义<strong>JsonExporterPipleline</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonExporterPipleline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 调用scrapy提供的json_export导出json文件</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'articleexport.json'</span>, <span class="string">'wb'</span>)</span><br><span class="line">        self.exporter = JsonItemExporter(self.file, encoding=<span class="string">"utf-8"</span>, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line">        self.exporter.start_exporting()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spoder</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.exporter.finish_exporting()</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.exporter.export_item(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>最后，在<strong>setting.py</strong>中开启管道<strong>JsonWithEncodingPipeline</strong>或<strong>JsonExporterPipleline</strong>。</p>
<p>Exporter中还提供有存储成其他类型文件的方式，可以Ctrl+自行查看。</p>
<h4 id="存入数据库-1"><a href="#存入数据库-1" class="headerlink" title="存入数据库"></a>存入数据库</h4><p>数据表设计：</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>类型</th>
<th>空</th>
<th>默认</th>
<th>注释</th>
</tr>
</thead>
<tbody>
<tr>
<td>title</td>
<td>varchar(200)</td>
<td>否</td>
<td></td>
<td></td>
</tr>
<tr>
<td>create_date</td>
<td>date</td>
<td>是</td>
<td><em>NULL</em></td>
<td></td>
</tr>
<tr>
<td>url</td>
<td>varchar(300)</td>
<td>否</td>
<td></td>
<td></td>
</tr>
<tr>
<td>url_object_id</td>
<td>varchar(50)</td>
<td>否</td>
<td></td>
<td></td>
</tr>
<tr>
<td>front_image_url</td>
<td>varchar(300)</td>
<td>是</td>
<td><em>NULL</em></td>
<td></td>
</tr>
<tr>
<td>front_image_path</td>
<td>varchar(200)</td>
<td>是</td>
<td><em>NULL</em></td>
<td></td>
</tr>
<tr>
<td>praise_nums</td>
<td>int(11)</td>
<td>是</td>
<td><em>NULL</em></td>
<td></td>
</tr>
<tr>
<td>collection_nums</td>
<td>int(11)</td>
<td>是</td>
<td><em>NULL</em></td>
<td></td>
</tr>
<tr>
<td>comment_nums</td>
<td>int(11)</td>
<td>是</td>
<td><em>NULL</em></td>
<td></td>
</tr>
<tr>
<td>tags</td>
<td>varchar(200)</td>
<td>是</td>
<td><em>NULL</em></td>
<td></td>
</tr>
<tr>
<td>content</td>
<td>longtext</td>
<td>是</td>
<td><em>NULL</em></td>
</tr>
</tbody>
</table>
<p>方法设计：</p>
<p>首先，引入MySQL：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> MySQLdb</span><br></pre></td></tr></table></figure>
<h5 id="同步机制"><a href="#同步机制" class="headerlink" title="同步机制"></a>同步机制</h5><p>在<strong>pipelines.py</strong>中定义<strong>MysqlPipeline</strong>，自定义的方法是同步机制，同步数据入库，速度慢，易堵塞。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 采用同步的机制,将数据存入数据库,方法一,同步入库,效率低,插入数据库速度跟不上爬取速度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.conn = MySQLdb.connect(<span class="string">'127.0.0.1'</span>, <span class="string">'article_spider'</span>, <span class="string">'7dKwHJYkXG'</span>, <span class="string">'article_spider'</span>, charset=<span class="string">"utf8"</span>,</span><br><span class="line">                                    use_unicode=<span class="keyword">True</span>)</span><br><span class="line">        self.cursor = self.conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        insert_sql = <span class="string">"insert into jobbole_article(title,create_date,url,url_object_id,front_image_url,front_image_path,praise_nums,collection_nums,comment_nums,tags,content)VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"</span></span><br><span class="line">        self.cursor.execute(insert_sql, (</span><br><span class="line">            item[<span class="string">'title'</span>], item[<span class="string">'create_date'</span>], item[<span class="string">'url'</span>], item[<span class="string">'url_object_id'</span>], item[<span class="string">'front_image_url'</span>][<span class="number">0</span>],</span><br><span class="line">            item[<span class="string">'front_image_path'</span>],</span><br><span class="line">            item[<span class="string">'praise_nums'</span>], item[<span class="string">'collection_nums'</span>], item[<span class="string">'comment_nums'</span>], item[<span class="string">'tags'</span>], item[<span class="string">'content'</span>]))</span><br><span class="line">        self.conn.commit()</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<h5 id="异步机制"><a href="#异步机制" class="headerlink" title="异步机制"></a>异步机制</h5><p>需要先引入cursors</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> MySQLdb.cursors</span><br></pre></td></tr></table></figure>
<p>在<strong>pipelines.py</strong>中定义<strong>MysqlTwistedPipeline</strong>，使用异步方式可以更加迅速地进行对数据库的插入。</p>
<p>先在<strong>setting.py</strong>中配置好数据库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataBase config</span></span><br><span class="line"><span class="comment"># 局域网</span></span><br><span class="line"><span class="comment"># MYSQL_HOST = "xxx.xxx.xxx.xxx"</span></span><br><span class="line"><span class="comment"># 本地</span></span><br><span class="line">MYSQL_HOST = <span class="string">"127.0.0.1"</span></span><br><span class="line">MYSQL_DBNAME = <span class="string">"article_spider"</span></span><br><span class="line">MYSQL_USER = <span class="string">"article_spider"</span></span><br><span class="line">MYSQL_PASSWORD = <span class="string">"xxxxxxxxxx"</span></span><br></pre></td></tr></table></figure>
<p><strong>MysqlTwistedPipeline</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlTwistedPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 采用异步的机制,将数据存入数据库,方法二，Twiste异步入库,先在settings中配置好数据库配置</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dbpool)</span>:</span></span><br><span class="line">        self.dbpool = dbpool</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">        dbparms = dict(</span><br><span class="line">            host=settings[<span class="string">"MYSQL_HOST"</span>],</span><br><span class="line">            db=settings[<span class="string">"MYSQL_DBNAME"</span>],</span><br><span class="line">            user=settings[<span class="string">"MYSQL_USER"</span>],</span><br><span class="line">            password=settings[<span class="string">"MYSQL_PASSWORD"</span>],</span><br><span class="line">            charset=<span class="string">"utf8"</span>,</span><br><span class="line">            cursorclass=MySQLdb.cursors.DictCursor,</span><br><span class="line">            use_unicode=<span class="keyword">True</span>,</span><br><span class="line">        )</span><br><span class="line">        dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparms)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(dbpool)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 使用twisted将mysql插入变成异步执行</span></span><br><span class="line">        query = self.dbpool.runInteraction(self.do_insert, item)</span><br><span class="line">        query.addErrback(self.handle_error)  <span class="comment"># 处理异常</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handle_error</span><span class="params">(self, failure)</span>:</span></span><br><span class="line">        <span class="comment"># 处理异步插入的异常</span></span><br><span class="line">        print(failure)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">do_insert</span><span class="params">(self, cursor, item)</span>:</span></span><br><span class="line">        <span class="comment"># 执行具体的插入</span></span><br><span class="line">        insert_sql = <span class="string">"insert into jobbole_article(title,create_date,url,url_object_id,front_image_url,front_image_path,praise_nums,collection_nums,comment_nums,tags,content)VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"</span></span><br><span class="line">        cursor.execute(insert_sql, (</span><br><span class="line">            item[<span class="string">'title'</span>], item[<span class="string">'create_date'</span>], item[<span class="string">'url'</span>], item[<span class="string">'url_object_id'</span>], item[<span class="string">'front_image_url'</span>][<span class="number">0</span>],</span><br><span class="line">            item[<span class="string">'front_image_path'</span>],</span><br><span class="line">            item[<span class="string">'praise_nums'</span>], item[<span class="string">'collection_nums'</span>], item[<span class="string">'comment_nums'</span>], item[<span class="string">'tags'</span>], item[<span class="string">'content'</span>]))</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>其中，使用python自带的dict可以轻松获取setting中配置好的数据库信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">    dbparms = dict(</span><br><span class="line">        host=settings[<span class="string">"MYSQL_HOST"</span>],</span><br><span class="line">        db=settings[<span class="string">"MYSQL_DBNAME"</span>],</span><br><span class="line">        user=settings[<span class="string">"MYSQL_USER"</span>],</span><br><span class="line">        password=settings[<span class="string">"MYSQL_PASSWORD"</span>],</span><br><span class="line">        charset=<span class="string">"utf8"</span>,</span><br><span class="line">        cursorclass=MySQLdb.cursors.DictCursor,</span><br><span class="line">        use_unicode=<span class="keyword">True</span>,</span><br><span class="line">    )</span><br><span class="line">    dbpool = adbapi.ConnectionPool(<span class="string">"MySQLdb"</span>, **dbparms)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cls(dbpool)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意**dbparms的传入方式</p>
</blockquote>
<p>对于插入数据库的方法，只需要更改<strong>do_insert</strong>即可。</p>
<h2 id="使用ItemLoader规范化解析"><a href="#使用ItemLoader规范化解析" class="headerlink" title="使用ItemLoader规范化解析"></a>使用ItemLoader规范化解析</h2><p>之前的爬取是通过response将xpath或者css选择器解析的字段存入局部变量，然后再通过ietm传入管道进行后续处理。但是由于其不规范性以及不利于后期维护，我们引入ItemLoader来进行规范化解析。</p>
<p>首先，在<strong>jobboleList.py</strong>中引入ItemLoader：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br></pre></td></tr></table></figure>
<p>ItemLoader对象有几种add方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">item_loader.add_xpath()</span><br><span class="line">item_loader.add_css()</span><br><span class="line">item_loader.add_value()</span><br></pre></td></tr></table></figure>
<p>用这其中的方法，可以直接将解析结果交给item</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文章封面图</span></span><br><span class="line">front_image_url = response.meta.get(<span class="string">"front_image_url"</span>, <span class="string">""</span>)</span><br><span class="line"><span class="comment"># 通过item loader加载实例</span></span><br><span class="line">item_loader = ArticleItemLoader(item=JobBoleArticleItem(), response=response)</span><br><span class="line"><span class="comment"># item_loader.add_xpath()</span></span><br><span class="line">item_loader.add_css(<span class="string">"title"</span>, <span class="string">".entry-header h1::text"</span>)</span><br><span class="line">item_loader.add_value(<span class="string">"url"</span>, response.url)</span><br><span class="line">item_loader.add_value(<span class="string">"url_object_id"</span>, response.url)</span><br><span class="line">item_loader.add_css(<span class="string">"create_date"</span>, <span class="string">".entry-meta-hide-on-mobile::text"</span>)</span><br><span class="line">item_loader.add_value(<span class="string">"front_image_url"</span>, [front_image_url])</span><br><span class="line">item_loader.add_css(<span class="string">"praise_nums"</span>, <span class="string">".vote-post-up h10::text"</span>)</span><br><span class="line">item_loader.add_css(<span class="string">"collection_nums"</span>, <span class="string">".bookmark-btn::text"</span>)</span><br><span class="line">item_loader.add_css(<span class="string">"comment_nums"</span>, <span class="string">"a[href='#article-comment'] span::text"</span>)</span><br><span class="line">item_loader.add_css(<span class="string">"tags"</span>, <span class="string">"p.entry-meta-hide-on-mobile a::text"</span>)</span><br><span class="line">item_loader.add_css(<span class="string">"content"</span>, <span class="string">"div.entry"</span>)</span><br><span class="line"></span><br><span class="line">article_item = item_loader.load_item()</span><br></pre></td></tr></table></figure>
<p>不过由于这种方式缺少了对字段的处理和过滤，比如时间对象转化，nums提取数值等。</p>
<p>再在<strong>ietm.py</strong>中对传入的值进行处理。</p>
<p>首先，引入MapCompose、TakeFirst及Join：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> MapCompose, TakeFirst, Join</span><br></pre></td></tr></table></figure>
<p>再在之前定义好的<strong>JobBoleArticleItem</strong>中对Field进行添加传入的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JobBoleArticleItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    create_date = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(date_filter, date_convert)</span><br><span class="line">    )</span><br><span class="line">    url = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_md5)</span><br><span class="line">    )</span><br><span class="line">    url_object_id = scrapy.Field()</span><br><span class="line">    front_image_url = scrapy.Field(</span><br><span class="line">        <span class="comment"># 由于传递的是数组，default获取的是第一个。将其覆盖可以用小技巧，传入一个函数，什么都不操作，只return value。</span></span><br><span class="line">        output_processor=MapCompose(return_value)</span><br><span class="line">    )</span><br><span class="line">    front_image_path = scrapy.Field()</span><br><span class="line">    praise_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    collection_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    comment_nums = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(get_nums)</span><br><span class="line">    )</span><br><span class="line">    tags = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_comment_tags),</span><br><span class="line">        output_processor=Join(<span class="string">','</span>)</span><br><span class="line">    )</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>input_processor：对传入字段进行处理。</p>
<p>output_processor：对传出字段进行处理。</p>
<p>由于爬取的是以list数据类型传入的，需要进行预处理，代替之前的extract_first()或extract()[0]，重新定义ItemLoader：</p>
<p>先引入ItemLoader：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br></pre></td></tr></table></figure>
<p><strong>ArticleItemLoader</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    <span class="comment"># 自定义ItemLoader</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br></pre></td></tr></table></figure>
<p><strong>各种处理函数</strong>：</p>
<p>注意需要先引入对应的库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> ArticleSpider.utils.common <span class="keyword">import</span> get_md5</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">date_filter</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value.strip().replace(<span class="string">"·"</span>, <span class="string">""</span>).strip()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">date_convert</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        value = datetime.datetime.strptime(value, <span class="string">"%Y/%m/%d"</span>).date()</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        value = datetime.datetime.now().date()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_nums</span><span class="params">(value)</span>:</span></span><br><span class="line">    match_re = re.match(<span class="string">".*?(\d+).*"</span>, value)</span><br><span class="line">    <span class="keyword">if</span> match_re:</span><br><span class="line">        nums = int(match_re.group(<span class="number">0</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        nums = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nums</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_comment_tags</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="comment"># 去掉tag中提取的评论</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">"评论"</span> <span class="keyword">in</span> value:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">return_value</span><span class="params">(value)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure>
<p>注意上面的front_image_url进行了处理，需要对<strong>pipelines.py</strong>的<strong>ArticleImagePipeline</strong>进行改动：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticleImagePipeline</span><span class="params">(ImagesPipeline)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span><span class="params">(self, results, item, info)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">"front_image_url"</span> <span class="keyword">in</span> item:</span><br><span class="line">            <span class="keyword">for</span> ok, value <span class="keyword">in</span> results:</span><br><span class="line">                image_file_path = value[<span class="string">"path"</span>]</span><br><span class="line">            item[<span class="string">"front_image_path"</span>] = image_file_path</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>判断是否存在front_image_url，如果不做判断，会报错。</p>
</div><div class="tags"><a href="/tags/spider/">spider</a></div><div class="post-nav"><a class="pre" href="/2018/06/06/scrapy反爬虫策略/">scrapy反爬虫策略</a><a class="next" href="/2018/05/22/MOOC-ThinkPHP模型篇-2数据库操作/">MOOC-ThinkPHP模型篇-2数据库操作</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/ThinkPHP/" style="font-size: 15px;">ThinkPHP</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/spider/" style="font-size: 15px;">spider</a> <a href="/tags/vue/" style="font-size: 15px;">vue</a> <a href="/tags/sql/" style="font-size: 15px;">sql</a> <a href="/tags/common/" style="font-size: 15px;">common</a> <a href="/tags/文献推荐/" style="font-size: 15px;">文献推荐</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/07/18/Git/">Git</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/16/Vue爬坑/">Vue爬坑</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/12/scrapy分布式爬虫/">scrapy分布式爬虫</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/07/08/py-sql/">py-sql</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/19/高数叔-大物/">高数叔-大物</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/17/常用sql语句/">常用sql语句</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/17/认识SCI、EI、ISTP、SSCI、INSPEC、SCIE、IEEE、CSCD、CSSCI/">认识SCI、EI、ISTP、SSCI、INSPEC、SCIE、IEEE、CSCD、CSSCI</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/13/scrapy进阶开发/">scrapy进阶开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/06/scrapy反爬虫策略/">scrapy反爬虫策略</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/31/scrapy爬取知名技术文章网站/">scrapy爬取知名技术文章网站</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Yaoyinnan's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>