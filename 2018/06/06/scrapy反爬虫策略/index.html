<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>scrapy反爬虫策略 | Yaoyinnan's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">scrapy反爬虫策略</h1><a id="logo" href="/.">Yaoyinnan's Blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">scrapy反爬虫策略</h1><div class="post-meta">Jun 6, 2018</div><div class="post-content"><h2 id="scrapy框架图"><a href="#scrapy框架图" class="headerlink" title="scrapy框架图"></a>scrapy框架图</h2><p><img src="/2018/06/06/scrapy反爬虫策略/1.png" alt="scrapy框架图1"></p>
<p><img src="/2018/06/06/scrapy反爬虫策略/2.png" alt="scrapy框架图2"></p>
<h4 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h4><p><strong>①引擎(Scrapy)</strong></p>
<p><strong>②调度器(Scheduler)</strong></p>
<p><strong>③下载器(Downloader)</strong></p>
<p><strong>④爬虫(Spiders)</strong></p>
<p><strong>⑤项目管道(Pipeline)</strong></p>
<p><strong>⑥下载器中间件(Downloader Middlewares)</strong></p>
<p><strong>⑦爬虫中间件(Spider Middlewares)</strong></p>
<p><strong>⑧调度中间件(Scheduler Middewares)</strong></p>
<h4 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h4><ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器</li>
<li>下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>
<li>解析出的是链接（URL）,则把URL交给调度器等待抓取</li>
</ol>
<h2 id="Request类-和-Response类"><a href="#Request类-和-Response类" class="headerlink" title="Request类 和 Response类"></a>Request类 和 Response类</h2><p>resquest由spiders产生</p>
<p>response由downloader产生</p>
<p>具体函数用法及其参数可以通过查看源码和文档了解。</p>
<h2 id="User-Agent用户代理"><a href="#User-Agent用户代理" class="headerlink" title="User Agent用户代理"></a>User Agent用户代理</h2><h4 id="User-Agent"><a href="#User-Agent" class="headerlink" title="User Agent"></a>User Agent</h4><p>在setting中设置可能现有的User Agent：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">user_agent_list = []</span><br></pre></td></tr></table></figure>
<p>在需要的文件中import。</p>
<p>通过生成随机数，来取user_agent：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random_index = random.randint(<span class="number">0</span>, len(user_agent_list) - <span class="number">1</span>)</span><br><span class="line">random_agent = user_agent_list[random_index]</span><br></pre></td></tr></table></figure>
<p>可以在需要使用的地方写该段代码，但缺少了代码复用。</p>
<p>可以通过github上一个开源的库<a href="https://github.com/hellysmile/fake-useragent" target="_blank" rel="noopener"><strong>fake-useragent</strong></a>来动态获取user-agent。</p>
<p>实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="comment"># 指定浏览器</span></span><br><span class="line">us.ie</span><br><span class="line">us.chrome</span><br><span class="line">us.firefox</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机浏览器</span></span><br><span class="line">us.random</span><br></pre></td></tr></table></figure>
<h4 id="Downloader-Middlewares-下载器中间件"><a href="#Downloader-Middlewares-下载器中间件" class="headerlink" title="Downloader  Middlewares(下载器中间件)"></a>Downloader  Middlewares(下载器中间件)</h4><p>由于上述的UserAgent的获取没进行代码复用，使用中间件可以进行设置。</p>
<p>可以在setting中配置Downloader Middlewares，默认的放在DownloaderMiddlewares类的useragent下。</p>
<p>自定义DownloaderMiddleware：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddlware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># 随即更换UserAgent</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, crawler)</span>:</span></span><br><span class="line">        super(RandomUserAgentMiddlware, self).__init__()</span><br><span class="line">        self.ua = UserAgent()</span><br><span class="line">        self.ua_type = crawler.settings.get(<span class="string">"RANDOM_US_THPE"</span>, <span class="string">"random"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(crawler)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_ua</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">return</span> getattr(self.ua, self.ua_type)</span><br><span class="line"></span><br><span class="line">        request.headers.setdefault(<span class="string">'User_Agent'</span>, get_ua())</span><br></pre></td></tr></table></figure>
<p>并且可以在setting中指定获得的ua的类型，并通过写闭包函数，用getattr方法将类型连接。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RANDOM_UA_TYPE = <span class="string">"random"</span></span><br></pre></td></tr></table></figure>
<h2 id="IP代理池"><a href="#IP代理池" class="headerlink" title="IP代理池"></a>IP代理池</h2><p>​    对于亚马逊的服务器，可以重启动态分配IP（阿里云除外，为静态IP）。公司、家中的路由器提供的IP也是动态分配的。本机IP是爬取效果最好的（IP代理较慢），最好是控制好爬取速度，保证本地IP不被封禁。</p>
<p><img src="/2018/06/06/scrapy反爬虫策略/3.png" alt="代理IP原理"></p>
<h4 id="方法一：通过爬取西刺免费IP代理来获取IP代理。"><a href="#方法一：通过爬取西刺免费IP代理来获取IP代理。" class="headerlink" title="方法一：通过爬取西刺免费IP代理来获取IP代理。"></a>方法一：通过爬取<a href="http://www.xicidaili.com/nn/" target="_blank" rel="noopener">西刺免费IP代理</a>来获取IP代理。</h4><p>在项目根目录下新建文件夹<strong>tools</strong>，存放脚本文件<strong>crawl_xici_ip.py</strong>：</p>
<h5 id="连接数据库"><a href="#连接数据库" class="headerlink" title="连接数据库"></a>连接数据库</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conn = MySQLdb.connect(host=<span class="string">"xxx.xxx.xxx.xxx"</span>, user=<span class="string">"username"</span>, passwd=<span class="string">"password"</span>, db=<span class="string">"dbname"</span>, charset=<span class="string">"utf8"</span>)</span><br></pre></td></tr></table></figure>
<h5 id="爬取西刺免费ip代理"><a href="#爬取西刺免费ip代理" class="headerlink" title="爬取西刺免费ip代理"></a>爬取西刺免费ip代理</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crawl_ips</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 爬取西刺的免费ip代理</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"User-Agent"</span>: <span class="string">"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Mobile Safari/537.36"</span>&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3205</span>):</span><br><span class="line">        re = requests.get(<span class="string">"http://www.xicidaili.com/nn/&#123;0&#125;"</span>.format(i), headers=headers)</span><br><span class="line">        selector = Selector(text=re.text)</span><br><span class="line">        all_trs = selector.css(<span class="string">"#ip_list tr"</span>)</span><br><span class="line"></span><br><span class="line">        ip_list = []</span><br><span class="line">        <span class="keyword">for</span> tr <span class="keyword">in</span> all_trs[<span class="number">1</span>:]:</span><br><span class="line">            speed_str = tr.css(<span class="string">".bar::attr(title)"</span>).extract_first()</span><br><span class="line">            <span class="keyword">if</span> speed_str:</span><br><span class="line">                speed = float(speed_str.split(<span class="string">"秒"</span>)[<span class="number">0</span>])</span><br><span class="line">            all_texts = tr.css(<span class="string">"td::text"</span>).extract()</span><br><span class="line"></span><br><span class="line">            ip = all_texts[<span class="number">0</span>]</span><br><span class="line">            port = all_texts[<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># proxy_type = all_texts[5]</span></span><br><span class="line"></span><br><span class="line">            ip_list.append((ip, port, speed))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ip_info <span class="keyword">in</span> ip_list:</span><br><span class="line">            cursor.execute(</span><br><span class="line">                <span class="string">"insert proxy_ip(ip, port, speed, proxy_type) VALUES('&#123;0&#125;', '&#123;1&#125;', &#123;2&#125;, 'HTTP')"</span>.format(ip_info[<span class="number">0</span>], ip_info[<span class="number">1</span>], ip_info[<span class="number">2</span>])</span><br><span class="line">            )</span><br><span class="line">            conn.commit()</span><br></pre></td></tr></table></figure>
<h5 id="获取IP"><a href="#获取IP" class="headerlink" title="获取IP"></a>获取IP</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GetIP</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delete_ip</span><span class="params">(self, ip)</span>:</span></span><br><span class="line">        <span class="comment"># 从数据库中删除无效的ip</span></span><br><span class="line">        delete_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">            delete from xwd_proxy_ip where ip='&#123;ip&#125;'</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cursor.execute(delete_sql)</span><br><span class="line">        conn.commit()</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">judge_ip</span><span class="params">(self, ip, port)</span>:</span></span><br><span class="line">        <span class="comment"># 判断ip是否可用</span></span><br><span class="line">        http_url = <span class="string">"http://www.baidu.com"</span></span><br><span class="line">        proxy_url = <span class="string">"http://&#123;0&#125;:&#123;1&#125;"</span>.format(ip, port)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            proxy_dict = &#123;</span><br><span class="line">                <span class="string">"http"</span>: proxy_url</span><br><span class="line">            &#125;</span><br><span class="line">            response = requests.get(http_url, proxies=proxy_dict)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(<span class="string">"invalid ip and port"</span>)</span><br><span class="line">            self.delete_ip(ip)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            code = response.status_code</span><br><span class="line">            <span class="keyword">if</span> <span class="number">200</span> &lt;= code &lt; <span class="number">300</span>:</span><br><span class="line">                print(<span class="string">"effective ip"</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"invalid ip and port"</span>)</span><br><span class="line">                self.delete(ip)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_random_ip</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 从数据库中随机获取一个可用的IP</span></span><br><span class="line">        random_sql = <span class="string">"""</span></span><br><span class="line"><span class="string">            SELECT ip, port FROM xwd_proxy_ip</span></span><br><span class="line"><span class="string">            ORDER BY RAND()</span></span><br><span class="line"><span class="string">            LIMIT 1</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        result = cursor.execute(random_sql)</span><br><span class="line">        <span class="keyword">for</span> ip_info <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line">            ip = ip_info[<span class="number">0</span>]</span><br><span class="line">            port = ip_info[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            judge_re = self.judge_ip(ip, port)</span><br><span class="line">            <span class="keyword">if</span> judge_re:</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"http://&#123;0&#125;:&#123;1&#125;"</span>.format(ip, port)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> self.get_random_ip()</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(crawl_ips())</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    get_ip = GetIP()</span><br><span class="line">    get_ip.get_random_ip()</span><br></pre></td></tr></table></figure>
<p>注意一定要加上“if __name__ == “__main__“:”这条代码，否则在别的spider文件中调用时会执行下面两条代码。</p>
<blockquote>
<p>可以从GitHub上的项目<strong><a href="https://github.com/aivarsk/scrapy-proxies" target="_blank" rel="noopener">scrapy-proxies</a></strong>中得到更完善的获取IP代理的脚本。</p>
</blockquote>
<p>在DownloadMiddleware中添加获取IP的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomProxyMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        get_ip = GetIP()</span><br><span class="line">        request.meta[<span class="string">"proxy"</span>] = get_ip.get_random_ip()</span><br></pre></td></tr></table></figure>
<h4 id="方法二：通过收费项目scrapy-crawlera来获取IP代理服务"><a href="#方法二：通过收费项目scrapy-crawlera来获取IP代理服务" class="headerlink" title="方法二：通过收费项目scrapy-crawlera来获取IP代理服务"></a>方法二：通过收费项目<a href="https://github.com/scrapy-plugins/scrapy-crawlera" target="_blank" rel="noopener"><strong>scrapy-crawlera</strong></a>来获取IP代理服务</h4><p>具体使用方法文档中有写，使用流程非常简单。</p>
<h4 id="方法三：通过洋葱网络Tor来包装IP"><a href="#方法三：通过洋葱网络Tor来包装IP" class="headerlink" title="方法三：通过洋葱网络Tor来包装IP"></a>方法三：通过<a href="http://www.theonionrouter.com/" target="_blank" rel="noopener"><strong>洋葱网络Tor</strong></a>来包装IP</h4><p>使用洋葱网络可以将自己的IP地址进行包装，达到匿名爬取网站的效果，而且效率要比网上免费IP代理甚至收费IP代理更好。主要问题是需要VPN。</p>
<h2 id="验证码识别"><a href="#验证码识别" class="headerlink" title="验证码识别"></a>验证码识别</h2><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>无需自己编写验证码识别代码，由于很多识别工具是需要通过训练集训练的，并且自己实现非常复杂，效果也不好。</p>
<h4 id="常见方法"><a href="#常见方法" class="headerlink" title="常见方法"></a>常见方法</h4><p>编码实现（tessract-ocr）—— 识别率低</p>
<p>在线打码 —— 识别率90%</p>
<p>人工打码 —— 识别率基本100%</p>
<h4 id="在线打码"><a href="#在线打码" class="headerlink" title="在线打码"></a>在线打码</h4><p>可以使用在线付费打码平台，如<a href="http://www.yundama.com/" target="_blank" rel="noopener">云打码</a>可以很好的实现常见验证码识别。具体使用文档可自行查看官网。</p>
<h2 id="隐藏爬虫行为的配置"><a href="#隐藏爬虫行为的配置" class="headerlink" title="隐藏爬虫行为的配置"></a>隐藏爬虫行为的配置</h2><p>在setting中设置：</p>
<p>具体内容和详情可以参照<a href="https://doc.scrapy.org/en/latest/intro/tutorial.html" target="_blank" rel="noopener">scrapy官方文档</a><a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html" target="_blank" rel="noopener">（中文文档）</a>，在其中可以查询到setting详细信息。</p>
<h4 id="Cookie禁用"><a href="#Cookie禁用" class="headerlink" title="Cookie禁用"></a>Cookie禁用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">COOKIES_ENABLED = <span class="keyword">False</span></span><br></pre></td></tr></table></figure>
<h4 id="自动限速"><a href="#自动限速" class="headerlink" title="自动限速"></a>自动限速</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONCURRENT_REQUESTS = <span class="number">32</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p><a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/autothrottle.html" target="_blank" rel="noopener">文档</a></p>
<h2 id="Spider设置特定setting"><a href="#Spider设置特定setting" class="headerlink" title="Spider设置特定setting"></a>Spider设置特定setting</h2><p>在爬虫class信息中设置特定的setting：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></div><div class="tags"><a href="/tags/spider/">spider</a></div><div class="post-nav"><a class="pre" href="/2018/06/13/scrapy进阶开发/">scrapy进阶开发</a><a class="next" href="/2018/05/31/scrapy爬取知名技术文章网站(二)/">scrapy爬取知名技术文章网站（二）</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/ThinkPHP/" style="font-size: 15px;">ThinkPHP</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/spider/" style="font-size: 15px;">spider</a> <a href="/tags/文献推荐/" style="font-size: 15px;">文献推荐</a> <a href="/tags/common/" style="font-size: 15px;">common</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/06/13/scrapy进阶开发/">scrapy进阶开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/06/scrapy反爬虫策略/">scrapy反爬虫策略</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/31/scrapy爬取知名技术文章网站(二)/">scrapy爬取知名技术文章网站（二）</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/22/MOOC-ThinkPHP模型篇-2数据库操作/">MOOC-ThinkPHP模型篇-2数据库操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/20/MOOC-ThinkPHP基础篇-5-8模板的布局-包含和继承/">MOOC-ThinkPHP基础篇-5-8模板的布局 包含和继承</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/20/MOOC-ThinkPHP基础篇-5-6比较标签 5-7条件判断标签/">MOOC-ThinkPHP基础篇-5-6比较标签 5-7条件判断标签</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/20/MOOC-ThinkPHP基础篇-5-5模板循环标签/">MOOC-ThinkPHP基础篇-5-5模板循环标签</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/20/MOOC-ThinkPHP基础篇-5-4变量输出-调节器/">MOOC-ThinkPHP基础篇-5-4变量输出 调节器</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/20/MOOC-ThinkPHP基础篇-5-3系统变量原生标签/">MOOC-ThinkPHP基础篇-5-3系统变量原生标签</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/07/scrapy爬取知名技术文章网站(一)/">scrapy爬取知名技术文章网站(一)</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Yaoyinnan's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>